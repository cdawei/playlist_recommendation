{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorisation by Minimising the Top Push Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "Learn latent features of songs and playlists by factorising the song-playlist matrix and optimising the Top Push bipartite ranking loss (see [Method](#Method) for details).\n",
    "\n",
    "\n",
    "## Method\n",
    "\n",
    "Given a dataset with $M$ songs and $N$ playlists,\n",
    "let function \n",
    "$$\n",
    "f(m, n) = \\mathbf{s}_m^\\top \\mathbf{p}_n + b_m, \\ m \\in \\{1,\\dots,M\\}, \\, n \\in \\{1,\\dots,N\\},\n",
    "$$\n",
    "where $\\mathbf{s}_m \\in \\mathbb{R}^{D \\times 1}$ ($D > 0$) is the feature vector of song $m$ ,\n",
    "$\\mathbf{p}_n \\in \\mathbb{R}^{D \\times 1}$ is the feature vector of playlist $n$,\n",
    "and $b_m$ is the bias of song $m$.\n",
    "\n",
    "Intuitively, $f(m, n)$ measures the affinity between song $m$ and playlist $n$.\n",
    "\n",
    "We minimising the number of songs appeared in the playlist but are scored lower than the *highest* scored song which is not appeared in the playlist,\n",
    "$$\n",
    "\\min_{\\mathbf{s}, \\mathbf{p}, \\mathbf{b}} \\ \n",
    "\\frac{1}{N} \\sum_{n=1}^N \\frac{1}{M_+^n}\n",
    "\\sum_{m: y_m^n = 1} I\\left( f(m, n) \\le \\max_{m': y_{m'}^n = 0} f(m', n) \\right)\n",
    "$$\n",
    "where $M_+^n$ is the number of songs in playlist $n$, $y_m^n = 1$ means song $n$ appeared in playlist $n$ and $y_{m'}^n = 0$ means song $m'$ didn't appear in playlist $n$, $I(\\cdot)$ is the indicator function.\n",
    "\n",
    "This is known as the *Top Push Loss* for bipartite ranking problem, which was proposed in \n",
    "[Li et al. Top Rank Optimization in Linear Time, NIPS'14](https://arxiv.org/abs/1410.1462)\n",
    "\n",
    "\n",
    "## Practical concerns\n",
    "\n",
    "We use the *squared hinge loss* $\\ell(z) = [\\max(0, 1-z)]^2$ as the convex surrogate of the 0-1 loss (i.e., the indicator function $I(\\cdot)$) in the Top Push loss, and optimise the objective using mini-batch sub-gradient descent via tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from scipy.sparse import isspmatrix_csr\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAND_SEED = 0\n",
    "data_dir = 'data'\n",
    "fmftrain = os.path.join(data_dir, 'mftrain_mpd.pkl.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the song-playlist matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y, _ = pkl.load(gzip.open(fmftrain, 'rb'))\n",
    "assert isspmatrix_csr(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the feature dimension, the number of training epochs and batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dim = 200\n",
    "n_epochs = 65\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File to store the learned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fparam = os.path.join(data_dir, 'mf-%d-mpd.npy' % feature_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model was trained using 4 Tesla P100-SXM2-16GB GPUs for 65 epochs, each epoch took about 40 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, M = Y.shape\n",
    "D = feature_dim\n",
    "n_batches = int((N - 1) / batch_size) + 1\n",
    "\n",
    "# Inputs\n",
    "# Note that the shape of some inputs are fixed so that\n",
    "# the cost can be computed in a way supported by tensorflow\n",
    "pIx = tf.placeholder(dtype=tf.int32, shape=[batch_size])     # playlist indices in current batch\n",
    "posIx = tf.placeholder(dtype=tf.int32, shape=[None, 2])      # indices of positive entries in current batch\n",
    "Mplus = tf.placeholder(dtype=tf.float32, shape=[batch_size]) # number of songs in each playlist\n",
    "nPos = tf.placeholder(dtype=tf.int32)                        # total number of positive entries in current batch\n",
    "\n",
    "# Parameters\n",
    "tf.set_random_seed(RAND_SEED)\n",
    "S = tf.Variable(initial_value=1e-3 * tf.random_uniform([D, M]))  # latent features of songs\n",
    "b = tf.Variable(initial_value=1e-3 * tf.random_uniform([1, M]))  # latent features of playlists\n",
    "P = tf.Variable(initial_value=1e-3 * tf.random_uniform([N, D]))  # biases of songs\n",
    "\n",
    "# Compute cost\n",
    "Pb = tf.gather(params=P, indices=pIx, axis=0)  # Nb by D, Nb = batch_size\n",
    "T = tf.matmul(Pb, S) + b  # scores: Nb by M\n",
    "pos_vec = tf.gather_nd(params=T, indices=posIx)  # scores of positive entries\n",
    "\n",
    "Tn = T + tf.scatter_nd(shape=T.shape, indices=posIx, updates=tf.tile([-np.inf], [nPos]))  # mask positive entries\n",
    "max_negs = tf.reduce_max(Tn, axis=1)  # element `n`: maximum score of songs not in playlist `n` in current batch\n",
    "\n",
    "row_ix = posIx[:, 0]  # row indices of positive entries\n",
    "relu_vec = tf.nn.relu(tf.divide(1 - pos_vec + tf.gather(max_negs, row_ix), tf.gather(Mplus, row_ix)))\n",
    "cost = tf.reduce_sum(tf.multiply(relu_vec, relu_vec)) / tf.cast(T.shape[0], tf.float32)  # mean squared hinge loss\n",
    "\n",
    "# optimiser\n",
    "optimiser = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(cost)\n",
    "\n",
    "# train\n",
    "init_op = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for epoch in range(n_epochs):\n",
    "        rand_ix = np.random.permutation(N)\n",
    "        for nb in range(n_batches):\n",
    "            ix_start = nb * batch_size\n",
    "            ix_end = min((nb + 1) * batch_size, N)\n",
    "            indices = rand_ix[ix_start:ix_end]\n",
    "            \n",
    "            # make sure each batch is of the specified size\n",
    "            if len(indices) < batch_size:\n",
    "                indices = np.r_[indices, rand_ix[:batch_size - len(indices)]]\n",
    "\n",
    "            Y_nb = Y[indices, :]\n",
    "            Mplus_nb = Y_nb.sum(axis=1).A.reshape(-1)\n",
    "            Y_coo = Y_nb.tocoo()\n",
    "\n",
    "            _, J = sess.run([optimiser, cost],\n",
    "                            feed_dict={pIx: indices,\n",
    "                                       Mplus: Mplus_nb,\n",
    "                                       nPos: Y_coo.row.shape[0],\n",
    "                                       posIx: np.hstack([Y_coo.row.reshape(-1, 1), Y_coo.col.reshape(-1, 1)])})\n",
    "            print('%d / %d / %d: %g' % (epoch + 1, nb + 1, n_batches, J))\n",
    "\n",
    "    # save parameters\n",
    "    w = np.r_[S.eval(), b.eval(), P.eval()]\n",
    "    np.save(fparam, w, allow_pickle=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
